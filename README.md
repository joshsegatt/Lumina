# âœ¨ Lumina

<div align="center">

![Version](https://img.shields.io/badge/version-1.0.0--beta.1-blue)
![React Native](https://img.shields.io/badge/React%20Native-19.2.0-61dafb)
![TypeScript](https://img.shields.io/badge/TypeScript-5.8.2-3178c6)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/status-Production%20Ready-success)

**An AI-powered assistant to spark creativity and illuminate ideas.**

*On-device AI inference â€¢ Offline-first â€¢ Privacy-focused â€¢ Lightning-fast*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Architecture](#-architecture) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸŒŸ Overview

Lumina is a cutting-edge React Native mobile application that brings the power of Large Language Models (LLMs) directly to your device. Engage with specialized AI modes to generate ideas, polish content, or weave vivid descriptionsâ€”all without an internet connection.

### Why Lumina?

- **ğŸ”’ Privacy First**: Your conversations never leave your device
- **âš¡ Lightning Fast**: Native on-device inference with llama.rn
- **ğŸŒ Offline Capable**: Works without internet after model download
- **ğŸ¨ Specialized Modes**: Tailored AI assistants for different creative tasks
- **ğŸ“± Native Performance**: Built with React Native for iOS & Android

---

## âœ¨ Features

### ğŸ¤– On-Device AI
- **Native LLM Engine**: Powered by [llama.rn](https://github.com/mybigday/llama.rn) for blazing-fast inference
- **GGUF Model Support**: Use quantized models (Q4, Q6, Q8) optimized for mobile
- **Smart Model Management**: Download, validate, and cache models locally

### ğŸ­ Specialized AI Modes
- **ğŸ’¡ Idea Generator**: Brainstorm creative concepts and solutions
- **âœï¸ Content Polisher**: Refine and enhance your writing
- **ğŸ–¼ï¸ Visual Describer**: Create vivid, detailed descriptions
- *(More modes coming soon!)*

### ğŸ›¡ï¸ Production-Grade Reliability
- **Error Boundaries**: Graceful error handling prevents app crashes
- **Download Retry Logic**: Exponential backoff for network failures
- **Race Condition Protection**: Mutex guards prevent concurrent operations
- **Memory Optimized**: Proper cleanup and leak prevention
- **Comprehensive Logging**: Structured logs for debugging

### ğŸŒ Internationalization
- **6 Languages**: English, Portuguese, Spanish, French, German, Japanese, Chinese
- **RTL Support**: Right-to-left languages ready
- **Easy Extension**: Simple i18n system for adding new locales

---

## ğŸš€ Installation

### Prerequisites

- **Node.js** 18+ and npm
- **React Native CLI** tools
- **Android Studio** (for Android) or **Xcode** (for iOS)
- **Git**

### Quick Start

```bash
# Clone the repository
git clone https://github.com/joshsegatt/Lumina.git
cd Lumina

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env
# Edit .env and add your HuggingFace token (optional, for faster downloads)

# Sync with Capacitor
npx cap sync

# Run on Android
npx cap open android
# Build in Android Studio

# OR Run on iOS (coming soon)
npx cap open ios
# Build in Xcode
```

### Environment Setup

Create a `.env` file in the root directory:

```env
# Optional: HuggingFace token for authenticated model downloads
# Get yours at: https://huggingface.co/settings/tokens
VITE_HF_TOKEN=hf_YOUR_TOKEN_HERE

# Optional: Gemini API key for cloud features (future)
VITE_GEMINI_API_KEY=your_gemini_key_here
```

---

## ğŸ“± Usage

### First Launch

1. **Select a Model**: Choose from pre-configured GGUF models
   - `Gemma 2B Q6_K` (~1.7GB) - Fast, good quality
   - `Phi-2 Q6_K` (~2.1GB) - Balanced performance
   - Or load your own GGUF model from device

2. **Download & Initialize**: The app will download and validate the model
   - Progress tracking with detailed status
   - Automatic retry on network failures
   - SHA256 validation for integrity

3. **Start Chatting**: Select a mode and begin your conversation
   - Real-time streaming responses
   - Token-per-second metrics
   - Conversation history saved locally

### Model Management

```typescript
// Models are cached in:
// Android: /data/data/com.lumina.app/files/models/
// iOS: <App>/Documents/models/

// Clear cache from Settings to free up space
```

---

## ğŸ—ï¸ Architecture

### Tech Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         React Native UI Layer       â”‚
â”‚  (TypeScript + Functional Components)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Service Layer                 â”‚
â”‚  â€¢ llmServiceAdapter                â”‚
â”‚  â€¢ nativeLlmService (llama.rn)      â”‚
â”‚  â€¢ conversationManager              â”‚
â”‚  â€¢ ModelDownloader                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Native Layer                  â”‚
â”‚  â€¢ llama.rn (C++ inference engine)  â”‚
â”‚  â€¢ React Native FS                  â”‚
â”‚  â€¢ AsyncStorage                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

#### `nativeLlmService.ts`
Native LLM manager with mutex-protected initialization:
- Pre-initialization model validation (file, size, hash)
- Real native error extraction (no more "Unknown" errors)
- Streaming text generation with token tracking

#### `ModelDownloader.ts`
Robust download manager:
- Exponential backoff retry logic (3 attempts)
- SHA256 integrity validation
- Chunked hash calculation for large files
- Smart error detection (404, 403, content-type)

#### `ErrorBoundary.tsx`
UI crash protection:
- Catches React rendering errors
- Displays user-friendly fallback UI
- Reset functionality to recover

#### `conversationManager.ts`
Persistent chat history:
- AsyncStorage for React Native compatibility
- Max 100 conversations limit
- Async-compatible callbacks

---

## ğŸ”§ Development

### Project Structure

```
lumina/
â”œâ”€â”€ components/          # React components
â”‚   â”œâ”€â”€ ChatWindow.tsx
â”‚   â”œâ”€â”€ ErrorBoundary.tsx
â”‚   â”œâ”€â”€ Header.tsx
â”‚   â””â”€â”€ ...
â”œâ”€â”€ services/           # Business logic
â”‚   â”œâ”€â”€ nativeLlmService.ts
â”‚   â”œâ”€â”€ llmServiceAdapter.ts
â”‚   â”œâ”€â”€ ModelDownloader.ts
â”‚   â”œâ”€â”€ conversationManager.ts
â”‚   â””â”€â”€ i18n.ts
â”œâ”€â”€ android/            # Android native project
â”œâ”€â”€ constants.tsx       # App configuration
â”œâ”€â”€ types.ts           # TypeScript definitions
â””â”€â”€ version.ts         # Build versioning
```

### Build Commands

```bash
# Development server
npm run dev

# Production build
npm run build

# Type checking
npx tsc --noEmit

# Sync with Capacitor
npx cap sync
```

### Logging System

Structured logging with consistent prefixes:

```typescript
[FLOW]         - High-level workflow state transitions
[EngineInit]   - Engine initialization details
[EngineError]  - Native error extraction
[Validator]    - Model validation (size, hash)
[Download]     - Download operations
[Chat]         - Message generation tracking
```

Filter logs in Android Studio:
```bash
adb logcat | grep -E "\[FLOW\]|\[EngineInit\]|\[Chat\]"
```

---

## ğŸ§ª Testing

### Verification Checklist

Run the comprehensive test suite documented in:
- **[VERIFICATION_GUIDE.md](VERIFICATION_GUIDE.md)** - 12-point verification checklist

Key tests:
- âœ… Build versioning confirms fresh bundle
- âœ… Model validation runs before engine start
- âœ… Real native errors (not "Unknown")
- âœ… Mutex prevents concurrent loads
- âœ… Token/s performance tracking works
- âœ… Memory cleanup on unmount
- âœ… Download retry on network failure

### QA Testing

Full QA checklist with 111 test cases available in:
- **[QA_CHECKLIST_RELEASE.md](QA_CHECKLIST_RELEASE.md)**

---

## ğŸ“š Documentation

Comprehensive documentation available:

- **[PRODUCTION_FIX_COMPLETE.md](PRODUCTION_FIX_COMPLETE.md)** - Technical implementation guide
- **[LAUNCH_READY_SUMMARY.md](LAUNCH_READY_SUMMARY.md)** - Executive summary
- **[VERIFICATION_GUIDE.md](VERIFICATION_GUIDE.md)** - Testing checklist
- **[GLOBAL_AUDIT_REPORT.md](GLOBAL_AUDIT_REPORT.md)** - Complete code audit
- **[LOGGING_GUIDE.md](LOGGING_GUIDE.md)** - Structured logging reference
- **[ROLLBACK_PLAN.md](ROLLBACK_PLAN.md)** - Recovery procedures

---

## ğŸ”’ Security

### Best Practices

- âœ… **No Secrets in Code**: All tokens in environment variables
- âœ… **Gitignore Configured**: `.env` files excluded from version control
- âœ… **GitHub Push Protection**: Prevents accidental token commits
- âœ… **Local Storage Only**: Conversations stored in AsyncStorage
- âœ… **No Cloud Sync**: Your data stays on your device

### Responsible Disclosure

Found a security issue? Please create a private security advisory.

---

## ğŸ¤ Contributing

We welcome contributions! Here's how:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit your changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### Contribution Guidelines

- Follow existing code style (TypeScript, functional components)
- Add tests for new features
- Update documentation
- Ensure 0 TypeScript errors: `npx tsc --noEmit`
- Test on real devices (Android/iOS)

---

## ğŸ“Š Roadmap

### v1.0.0 (Current - Beta)
- âœ… On-device inference with llama.rn
- âœ… 3 specialized AI modes
- âœ… Model download and caching
- âœ… 6 language support
- âœ… Comprehensive error handling

### v1.1.0 (Q1 2026)
- ğŸ”„ Additional AI modes (Code Assistant, Translator)
- ğŸ”„ Model size optimization
- ğŸ”„ iOS support
- ğŸ”„ Widget support

### v2.0.0 (Q2 2026)
- ğŸ”® Multi-model support (load multiple models)
- ğŸ”® Function calling capabilities
- ğŸ”® Voice input/output
- ğŸ”® Advanced conversation features (search, export)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with amazing open-source technologies:

- [React Native](https://reactnative.dev/) - Mobile framework
- [llama.rn](https://github.com/mybigday/llama.rn) - On-device LLM inference
- [Capacitor](https://capacitorjs.com/) - Native runtime
- [TypeScript](https://www.typescriptlang.org/) - Type safety
- [Vite](https://vitejs.dev/) - Build tool

Special thanks to:
- HuggingFace for hosting GGUF models
- The llama.cpp community
- All open-source contributors

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/joshsegatt/Lumina/issues)
- **Discussions**: [GitHub Discussions](https://github.com/joshsegatt/Lumina/discussions)

---

<div align="center">

**Made with â¤ï¸ for the AI community**

â­ Star us on GitHub if you find this project useful!

</div>
